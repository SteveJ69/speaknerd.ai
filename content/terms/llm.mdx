---
title: "LLM"
subtitle: "Large Language Model"
category: "ai-basics"
difficulty: "beginner"
tldr: "The brain behind ChatGPT. It read the entire internet and now it guesses what word comes next. Really, really well."
related: ["gpt", "token", "prompt", "hallucination"]
---

## The Plain English Version

Imagine you had a friend who read every book, article, and website ever written. Now imagine you could ask that friend anything, and they'd give you a pretty solid answer — not because they actually *understand* what they read, but because they've seen so many patterns that they can predict what a good answer looks like.

That's an LLM. It's a massive computer program trained on billions of words from the internet. When you type something into ChatGPT, the LLM reads your message and essentially goes, "Based on everything I've ever seen, the most likely next word is..." and it does that over and over, one word at a time, until it's built a full response.

It's not thinking. It's not conscious. It's pattern-matching at a scale that makes it look incredibly smart. And honestly? It works shockingly well.

## Why Should You Care?

Because LLMs are the engine behind almost every AI tool you're hearing about. ChatGPT, Claude, Gemini — they're all LLMs under the hood. Understanding what they are (and what they aren't) is the difference between using AI effectively and being confused by everything everyone says about it.

## The Nerd Version (if you dare)

An LLM is a neural network (usually a transformer architecture) trained via self-supervised learning on massive text corpora. During training, it learns to predict the next token in a sequence. The "large" part refers to the parameter count — modern LLMs have billions to trillions of parameters. They generate text autoregressively, producing one token at a time based on probability distributions.
