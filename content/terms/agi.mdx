---
title: "AGI"
subtitle: "Artificial General Intelligence"
category: "buzzwords"
difficulty: "beginner"
tldr: "AI that can do everything a human can. We don't have it yet. Anyone who says we do is selling something."
related: ["llm", "neural-network", "deep-learning"]
---

## The Plain English Version

Right now, AI is really good at specific things. ChatGPT is great at writing but can't drive a car. A self-driving AI can navigate roads but can't write a poem. Each AI is a specialist — amazing at one thing, useless at others.

AGI is the dream of building AI that can do everything a human can. Not just one task — all tasks. Learn new things on its own, reason about problems it's never seen, be creative, adapt, and generally think the way we do. Like a mind, not just a tool.

We don't have AGI. Not even close, depending on who you ask. What we have are really impressive narrow AI systems that sometimes feel general because they can handle lots of different text tasks. But there's a massive gap between "can write a good email" and "can genuinely think."

## Why Should You Care?

Because people throw "AGI" around like it's right around the corner, and it shapes investment decisions, policy debates, and a lot of fear. When you see a headline about AGI, you should know it's still theoretical. Current AI is incredibly useful, but it's tools — not minds. Don't let the hype scare you or inflate your expectations.

## The Nerd Version (if you dare)

AGI refers to hypothetical AI systems that match or exceed human cognitive abilities across all domains. No consensus exists on its definition, timeline, or even feasibility. Current LLMs demonstrate broad capabilities but lack persistent memory, genuine reasoning, embodiment, and autonomy. Researchers debate whether scaling current architectures can achieve AGI or if fundamentally new approaches are needed.
