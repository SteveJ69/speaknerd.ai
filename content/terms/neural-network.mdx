---
title: "Neural Network"
subtitle: "Loosely inspired by brains"
category: "buzzwords"
difficulty: "beginner"
tldr: "A computer system loosely inspired by how brain cells connect. It's how AI learns patterns. Don't worry — it's not actually a brain."
related: ["deep-learning", "llm", "agi"]
---

## The Plain English Version

Your brain has about 86 billion neurons connected by trillions of synapses. When you learn something — like recognizing your mom's face or catching a ball — your neurons form patterns. Do something enough times, and the pattern gets strong.

A neural network is a computer program that borrows this idea. It has artificial "neurons" arranged in layers. Data goes in one side, passes through layers of neurons that each do a tiny calculation, and a result comes out the other side. The magic is that the network can learn by adjusting the connections between neurons until it gets good at whatever task you give it.

Show it 10,000 pictures of cats and 10,000 pictures of dogs, and it'll learn to tell the difference. Show it millions of sentences, and it'll learn to write like a human. That's how LLMs work at their core — they're really big neural networks trained on really big datasets.

## Why Should You Care?

Because neural networks are the foundation of basically all modern AI. When someone says "machine learning" or "deep learning" or "AI model," they're almost always talking about some flavor of neural network. Understanding this concept is like understanding that cars have engines — you don't need to build one, but it helps to know it's there.

## The Nerd Version (if you dare)

Neural networks consist of interconnected nodes organized in layers (input, hidden, output). Each connection has a weight adjusted during training via backpropagation and gradient descent. Activation functions (ReLU, sigmoid, softmax) introduce non-linearity. Modern architectures include CNNs (images), RNNs/LSTMs (sequences), and Transformers (attention-based, used in LLMs).
