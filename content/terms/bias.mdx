---
title: "Bias"
subtitle: "When AI inherits our prejudices"
category: "ai-basics"
difficulty: "beginner"
tldr: "AI learns from human data, and humans are biased. So AI picks up those biases too. If you only show it photos of male CEOs, it assumes all CEOs are male."
related: ["training-data", "alignment", "machine-learning"]
---

## The Plain English Version

Here's an uncomfortable truth: AI is basically a mirror. It reflects whatever it was trained on — including all the biases, stereotypes, and unfairness baked into that data.

If an AI was trained on decades of hiring data where men were hired more often for engineering roles, it might learn that "engineer = male" and start penalizing female applicants. Not because anyone told it to be sexist — but because the historical data WAS sexist, and the AI learned the pattern. Amazon actually built a hiring AI that did exactly this. They had to scrap it.

It goes beyond gender. AI can be biased about race, age, geography, language, disability, and more. Facial recognition systems have been shown to work great on white faces and terribly on darker skin tones — because the training data was mostly white faces. The AI wasn't racist on purpose. It just never learned to see everyone equally.

## Why Should You Care?

Because biased AI makes real decisions about real people. Loan approvals, job screenings, medical diagnoses, criminal sentencing — AI is being used in all of these. If you're ever on the receiving end of an AI decision (and you will be), understanding bias helps you question results that don't seem right instead of blindly accepting the computer's answer.

## The Nerd Version (if you dare)

AI bias arises from multiple sources: training data bias (unrepresentative or historically skewed datasets), algorithmic bias (model architecture choices that amplify patterns), and deployment bias (using models in contexts they weren't designed for). Mitigation strategies include diverse and representative data collection, fairness-aware training objectives, bias auditing tools (AIF360, Fairlearn), red-teaming, and human-in-the-loop review for high-stakes decisions.
