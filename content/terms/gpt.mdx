---
title: "GPT"
subtitle: "Generative Pre-trained Transformer"
category: "ai-basics"
difficulty: "beginner"
tldr: "The model family behind ChatGPT. It's the engine — ChatGPT is the car you drive."
related: ["llm", "chatgpt", "token"]
---

## The Plain English Version

Think of GPT like an engine and ChatGPT like the car. GPT is the technology under the hood — it's the thing that actually generates text. ChatGPT is the nice interface OpenAI wrapped around it so regular people could use it.

GPT stands for "Generative Pre-trained Transformer," which sounds terrifying but breaks down simply. "Generative" means it creates new text. "Pre-trained" means it studied a massive amount of text before you ever talked to it. "Transformer" is the type of architecture it uses — basically the blueprint for how it processes language.

When people say "GPT-4" or "GPT-4o," they're talking about different versions of this engine. Each version is bigger, smarter, and better at understanding what you mean. It's like car models — a 2025 model is better than a 2020 model.

## Why Should You Care?

Because GPT is the technology that kicked off this entire AI revolution. When you hear people talking about "generative AI," GPT is what started it. Understanding what GPT is helps you understand why ChatGPT works the way it does — and why it sometimes doesn't.

## The Nerd Version (if you dare)

GPT is a decoder-only transformer model developed by OpenAI. It's trained using unsupervised learning on internet text, then fine-tuned with RLHF (Reinforcement Learning from Human Feedback). GPT-4 is multimodal, accepting both text and image inputs. The architecture uses attention mechanisms to process token sequences in parallel.
