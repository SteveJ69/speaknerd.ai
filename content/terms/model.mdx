---
title: "Model"
subtitle: "The trained brain of an AI"
category: "ai-basics"
difficulty: "beginner"
tldr: "The finished product after training — the 'brain' that does the thinking. GPT-4, Claude, Llama — those are all models. Billions of dollars of training, packaged up for you to use."
related: ["parameter", "llm", "fine-tuning"]
---

## The Plain English Version

Think of a model like a trained employee. You invested time and money training them (that's the training phase), and now they're ready to work. The "model" is the finished result — all the knowledge and capability, packaged up and ready to answer questions, generate text, or create images.

When people say "GPT-4" or "Claude" or "Llama," they're talking about specific models. Each one was trained differently, on different data, with different goals. Some are better at coding. Some are better at creative writing. Some are smaller and faster. Some are bigger and smarter. Picking the right model for your task is like picking the right employee for a job.

Here's the thing most people don't realize: the model is essentially frozen after training. GPT-4 isn't learning from your conversations in real-time. It's applying what it already learned during training. When a new version comes out (GPT-4 → GPT-5), that's a whole new model trained from scratch — not the old one that "got smarter."

## Why Should You Care?

Because not all models are created equal, and choosing the right one matters. Some are free, some are expensive. Some are fast, some are slow. Some are open-source (anyone can use them), some are proprietary (locked behind a company's API). Understanding what a model is helps you make smarter choices about which AI tools to use and why they cost what they cost.

## The Nerd Version (if you dare)

An AI model is a mathematical function with trained parameters that maps inputs to outputs. Models are defined by their architecture (transformer, CNN, etc.), parameter count, training data, and training objective. The model file (weights) contains all learned parameters serialized to disk (GGUF, safetensors, PyTorch checkpoints). Models can be fine-tuned, quantized, distilled, or merged. The model ecosystem includes foundation models (pre-trained), fine-tuned variants, and task-specific models.
