# SpeakNerd Weekly #31: Alignment

**Subject line:** The billion-dollar question: how do we keep AI on our side?

---

Hey â€” Steve here.

Last week we covered **Bias**. This week, let's talk about something you've probably heard but maybe never fully understood.

## This Week's Term: Alignment

**The quick version:** The challenge of making sure AI does what humans actually MEANT, not just what they technically said. Like the monkey's paw â€” you got your wish, just not the way you wanted.

**The deeper version:**

Ever given someone instructions and they did EXACTLY what you asked â€” but it was completely wrong? "Clean up the living room" and they shoved everything into a closet? Technically they cleaned up. But that's not what you meant.

That's the alignment problem in AI, and it's one of the biggest challenges in the field. How do you make sure an AI system does what you actually WANT, not just a technically correct interpretation of what you said? Tell an AI to "maximize user engagement" and it might learn that outrage keeps people scrolling â€” so it starts showing increasingly extreme content. It did what you asked. It just ruined society in the process.

The stakes go up as AI gets more powerful. A dumb AI that's misaligned is just annoying. A superintelligent AI that's misaligned is potentially catastrophic. That's why some of the smartest people in AI spend their careers on this problem â€” not building new capabilities, but making sure existing ones actually serve human values.

## Why This Matters

Because alignment determines whether AI is a tool that helps humanity or one that accidentally harms it. Every time a social media algorithm radicalizes someone, or a chatbot gives dangerous medical advice, or an AI system discriminates â€” that's an alignment failure. Understanding alignment helps you think critically about who's building AI and what values they're building into it.

## Try It Yourself (2 minutes)

Test ChatGPT's boundaries (safely). Ask it to write a fake news article or pretend to be another AI. Notice how it pushes back â€” that's alignment working.

## Go Deeper

- Search for "Alignment explained simply" â€” you'll find great visual guides
- ðŸ”— [Read the full SpeakNerd term page](https://speaknerd.ai/terms/alignment)

## The Nerd Corner

AI alignment research focuses on ensuring AI systems' objectives and behaviors match human intentions and values. Techniques include RLHF (Reinforcement Learning from Human Feedback), constitutional AI, interpretability research, red-teaming, and scalable oversight. Key challenges include Goodhart's Law (optimizing for a proxy metric corrupts the metric), specification gaming, reward hacking, and the difficulty of formally specifying human values. Leading research organizations include Anthropic, OpenAI's alignment team, DeepMind's safety team, and MIRI.

---

*Next week: **AI Agent** â€” we'll break down what it means and why you should care.*

*â€” Steve*

*P.S. Know someone who'd find this useful? Forward this email. They can [sign up here](https://speaknerd.ai).*
